{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majam001/miniconda3/envs/micromind/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from micromind.networks import phinet\n",
    "from micromind.utils import parse_configuration\n",
    "from recipes.image_classification.train import ImageClassification, top_k_accuracy\n",
    "from recipes.image_classification.prepare_data import create_loaders\n",
    "import micromind as mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters for each module: {'classifier': 4783591}\n",
      "Number of MAC for each module: {'classifier': 31426516}\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "hparams = parse_configuration(\"../../cfg/phinet.py\") \n",
    "mind = ImageClassification(hparams=hparams)\n",
    "loaders = create_loaders(hparams)\n",
    "top1 = mm.Metric(\"top1_acc\", top_k_accuracy(k=1), eval_only=True)\n",
    "top5 = mm.Metric(\"top5_acc\", top_k_accuracy(k=5), eval_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "hparams.h_swish=False\n",
    "print(hparams.h_swish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/majam001/thesis_mariam/micromind/micromind/utils/checkpointer.py:112: UserWarning: \"You did not specify the configuration to the checkpointer, so it won't be saved. You can pass one using the hparams argument. Ignore this if you are in debug mode.\"\n",
      "  warnings.warn(\" \".join(tmp.split()))\n"
     ]
    }
   ],
   "source": [
    "exp_configuration = \"hswish_false\"\n",
    "exp_folder = mm.utils.checkpointer.create_experiment_folder(\n",
    "    hparams.output_folder, hparams.experiment_name + \"+\" + exp_configuration\n",
    ")\n",
    "\n",
    "checkpointer = mm.utils.checkpointer.Checkpointer(\n",
    "    exp_folder,\n",
    "    key=\"loss\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-13 15:34:56\u001b[0m |             \u001b[1mINFO    \u001b[0m |              \u001b[1mRecovered from checkpoint results/micromind_exp+hswish_false/save/2025-01-08+09-46-22 at epoch 50.\u001b[0m\n",
      "\u001b[32m2025-01-13 15:34:56\u001b[0m |             \u001b[1mINFO    \u001b[0m |              \u001b[1mval_loss was 1.4153 for this checkpoint.\u001b[0m\n",
      "/home/majam001/miniconda3/envs/micromind/lib/python3.11/site-packages/accelerate/checkpointing.py:159: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  models[i].load_state_dict(torch.load(input_model_file, map_location=map_location), **load_model_func_kwargs)\n",
      "/home/majam001/miniconda3/envs/micromind/lib/python3.11/site-packages/accelerate/checkpointing.py:166: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer_state = torch.load(input_optimizer_file, map_location=map_location)\n",
      "/home/majam001/miniconda3/envs/micromind/lib/python3.11/site-packages/accelerate/checkpointing.py:185: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  states = torch.load(os.path.join(input_dir, f\"{RNG_STATE_NAME}_{process_index}.pkl\"))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of custom checkpoints in folder {input_dir} does not match the number of registered objects:\n\tFound checkpoints: 6\n\tRegistered objects: 2\nPlease make sure to only load checkpoints from folders that were created with the same set of registered objects,or avoid using `custom_checkpoint` in the filename for files in that same directory and load them in manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mind\u001b[39m.\u001b[39mtrain(\n\u001b[1;32m      2\u001b[0m     epochs\u001b[39m=\u001b[39mhparams\u001b[39m.\u001b[39mepochs,\n\u001b[1;32m      3\u001b[0m     datasets\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m: loaders[\u001b[39m0\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m: loaders[\u001b[39m1\u001b[39m]},\n\u001b[1;32m      4\u001b[0m     metrics\u001b[39m=\u001b[39m[top1, top5],\n\u001b[1;32m      5\u001b[0m     checkpointer\u001b[39m=\u001b[39mcheckpointer,\n\u001b[1;32m      6\u001b[0m     debug\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     verbose \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m test_results \u001b[39m=\u001b[39m mind\u001b[39m.\u001b[39mtest(\n\u001b[1;32m     11\u001b[0m     datasets\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m: loaders[\u001b[39m1\u001b[39m]}, metrics\u001b[39m=\u001b[39m[top1, top5], verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/thesis_mariam/micromind/micromind/core.py:489\u001b[0m, in \u001b[0;36mMicroMind.train\u001b[0;34m(self, epochs, datasets, metrics, checkpointer, verbose, debug)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39massert\u001b[39;00m epochs \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mYou must specify at least one epoch.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39m=\u001b[39m debug\n\u001b[0;32m--> 489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_train_start()\n\u001b[1;32m    491\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mis_local_main_process:\n\u001b[1;32m    492\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    493\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStarting from epoch \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_epoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    494\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Training is scheduled for \u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m epochs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m     )\n",
      "File \u001b[0;32m~/thesis_mariam/micromind/micromind/core.py:396\u001b[0m, in \u001b[0;36mMicroMind.on_train_start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[39mif\u001b[39;00m ckpt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m         accelerate_path, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_epoch \u001b[39m=\u001b[39m ckpt\n\u001b[0;32m--> 396\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mload_state(accelerate_path)\n\u001b[1;32m    397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     tmp \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    399\u001b[0m \u001b[39m        You are not passing a checkpointer to the training function, \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[39m        thus no status will be saved. If this is not the intended behaviour \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m        please check https://micromind-toolkit.github.io/docs/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m).\u001b[39m\n\u001b[1;32m    402\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/micromind/lib/python3.11/site-packages/accelerate/accelerator.py:2957\u001b[0m, in \u001b[0;36mAccelerator.load_state\u001b[0;34m(self, input_dir, **load_model_func_kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m     err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPlease make sure to only load checkpoints from folders that were created with the same set of registered objects,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2956\u001b[0m     err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mor avoid using `custom_checkpoint` in the filename for files in that same directory and load them in manually.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 2957\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(err)\n\u001b[1;32m   2958\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2959\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading in \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(custom_checkpoints)\u001b[39m}\u001b[39;00m\u001b[39m custom states\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Number of custom checkpoints in folder {input_dir} does not match the number of registered objects:\n\tFound checkpoints: 6\n\tRegistered objects: 2\nPlease make sure to only load checkpoints from folders that were created with the same set of registered objects,or avoid using `custom_checkpoint` in the filename for files in that same directory and load them in manually."
     ]
    }
   ],
   "source": [
    "mind.train(\n",
    "    epochs=hparams.epochs,\n",
    "    datasets={\"train\": loaders[0], \"val\": loaders[1]},\n",
    "    metrics=[top1, top5],\n",
    "    checkpointer=checkpointer,\n",
    "    debug=False,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "test_results = mind.test(\n",
    "    datasets={\"test\": loaders[1]}, metrics=[top1, top5], verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from recipes.image_classification.hpo.zero_shot_proxies.naswot import compute_nas_score\n",
    "from recipes.image_classification.hpo.zero_shot_proxies.synflow import compute_synflow_score\n",
    "from recipes.image_classification.hpo.zero_shot_proxies.te_nas_score import compute_TE_NAS_score\n",
    "from recipes.image_classification.hpo.zero_shot_proxies.zen_score import compute_zen_score\n",
    "#from recipes.image_classification.hpo.zero_shot_proxies.zico import compute_zen_score\n",
    "from recipes.image_classification.hpo.zero_shot_proxies.grad_norm import compute_gradnorm_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters for each module: {'classifier': 4783591}\n",
      "Number of MAC for each module: {'classifier': 31426516}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImageClassification(hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-13 15:45:10\u001b[0m |             \u001b[1mINFO    \u001b[0m |              \u001b[1mSuccessfully loaded model from checkpoint.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_modules('/home/majam001/thesis_mariam/micromind/recipes/image_classification/hpo/zero_shot_proxies/results/micromind_exp+hswish_false/save/2025-01-08+09-46-22/state-dict.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASWOT = 271.7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_timer = time.time()\n",
    "repeat_times = 32\n",
    "input_image_size = 32\n",
    "batch_size = 16\n",
    "\n",
    "for repeat_count in range(repeat_times):\n",
    "    naswot_score = compute_nas_score(gpu=0, model=model.modules[\"classifier\"].to(device),\n",
    "    #                     resolution=input_image_size, batch_size=batch_size)\n",
    "    # synflow_score = compute_synflow_score(gpu=0, model=model.modules[\"classifier\"].to(device),\n",
    "    #                     resolution=input_image_size, batch_size=batch_size)\n",
    "    # te_nas_score = compute_TE_NAS_score(gpu=0, model=model.modules[\"classifier\"].to(device),\n",
    "                        resolution=input_image_size, batch_size=batch_size)\n",
    "    # gradnorm_score = compute_gradnorm_score(gpu=0, model=model.modules[\"classifier\"].to(device),\n",
    "    #                   resolution=input_image_size, batch_size=batch_size)\n",
    "\n",
    "time_cost = (time.time() - start_timer) / repeat_times\n",
    "\n",
    "#zen_score = compute_zen_score(gpu=0, model=model.modules[\"classifier\"].to(device), mixup_gamma=1e-2, \n",
    "#                        resolution=input_image_size, batch_size=batch_size, repeat=32, fp16=False)['avg_nas_score']\n",
    "\n",
    "print(f'NASWOT = {naswot_score:.4g}')\n",
    "#print(f'Synflow ={synflow_score:.4g}')\n",
    "#print(f'TE_NAS = {te_nas_score:.4g}')\n",
    "#print(f'Gradnorm = {gradnorm_score:.4g}')\n",
    "#print(f'Zen ={zen_score:.4g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('micromind': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7e24afc8ad1867ba4e489ed98cb5062a454250f83694b3142a860203967e7c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
